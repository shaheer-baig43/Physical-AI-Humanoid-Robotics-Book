"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1963],{6568:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>o,toc:()=>r});const o=JSON.parse('{"id":"module-4-vla/cognitive-planning","title":"Cognitive Planning with LLMs","description":"A key part of Vision-Language-Action (VLA) models is the ability to perform cognitive planning. This involves using Large Language Models (LLMs) to translate high-level natural language commands into a sequence of concrete actions that a robot can execute.","source":"@site/docs/module-4-vla/cognitive-planning.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/cognitive-planning","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/shaheer-baig43/Physical-AI-Humanoid-Robotics-Book/tree/main/docs/module-4-vla/cognitive-planning.md","tags":[],"version":"current","frontMatter":{},"sidebar":"physicalAiSidebar","previous":{"title":"Voice-to-Action","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/voice-to-action"},"next":{"title":"Capstone Project: The Autonomous Humanoid","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/capstone-project"}}');var i=t(4848),a=t(8453);const s={},l="Cognitive Planning with LLMs",c={},r=[{value:"From Language to Actions",id:"from-language-to-actions",level:2}];function h(n){const e={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"cognitive-planning-with-llms",children:"Cognitive Planning with LLMs"})}),"\n",(0,i.jsxs)(e.p,{children:["A key part of Vision-Language-Action (VLA) models is the ability to perform ",(0,i.jsx)(e.strong,{children:"cognitive planning"}),". This involves using Large Language Models (LLMs) to translate high-level natural language commands into a sequence of concrete actions that a robot can execute."]}),"\n",(0,i.jsx)(e.h2,{id:"from-language-to-actions",children:"From Language to Actions"}),"\n",(0,i.jsx)(e.p,{children:'For example, a command like "Clean the room" is too abstract for a robot to understand directly. A cognitive planning system using an LLM would break this down into smaller steps:'}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Identify objects that are out of place (e.g., a cup on the table)."}),"\n",(0,i.jsx)(e.li,{children:"Find the correct location for the object (e.g., the kitchen sink)."}),"\n",(0,i.jsx)(e.li,{children:"Plan a path to the cup."}),"\n",(0,i.jsx)(e.li,{children:"Pick up the cup."}),"\n",(0,i.jsx)(e.li,{children:"Plan a path to the kitchen sink."}),"\n",(0,i.jsx)(e.li,{children:"Place the cup in the sink."}),"\n",(0,i.jsx)(e.li,{children:"Repeat for other objects."}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"This sequence of actions can then be translated into ROS 2 commands for the robot's navigation and manipulation systems."})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(h,{...n})}):h(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>l});var o=t(6540);const i={},a=o.createContext(i);function s(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);